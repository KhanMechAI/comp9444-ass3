{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Nerual Networks\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing Nerual Networks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (Neural_QTrain.py, line 40)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3265\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-73948a00e418>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    import Neural_QTrain\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/Users/khan/OneDrive - UNSW/University/Postgraduate/Year 1 Semester 2/COMP9444/Assignments/Assignment 3/src/Neural_QTrain.py\"\u001b[0;36m, line \u001b[0;32m40\u001b[0m\n\u001b[0;31m    q_values = tf.layers.dense(state_in, ACTION_DIM, kernel_initializer=tf.random_uniform_initializer(0,.01), activation=tf.nn.softmax)\u001b[0m\n\u001b[0m                                                                                                                                       ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# General Parameters\n",
    "# -- DO NOT MODIFY --\n",
    "ENV_NAME = 'CartPole-v0'\n",
    "EPISODE = 200000  # Episode limitation\n",
    "STEP = 200  # Step limitation in an episode\n",
    "TEST = 10  # The number of tests to run every TEST_FREQUENCY episodes\n",
    "TEST_FREQUENCY = 100  # Num episodes to run before visualizing test accuracy\n",
    "\n",
    "# TODO: HyperParameters\n",
    "GAMMA =  .8# discount factor\n",
    "INITIAL_EPSILON = .2# starting value of epsilon\n",
    "FINAL_EPSILON = .05 # final value of epsilon\n",
    "EPSILON_DECAY_STEPS = 1000# decay period\n",
    "HIDDEN_UNITS = 50\n",
    "\n",
    "BATCH_SIZE = 5\n",
    "batch_size = BATCH_SIZE\n",
    "DROP_PROB = 0.5\n",
    "dropout_keep_prob = DROP_PROB\n",
    "# Create environment\n",
    "# -- DO NOT MODIFY --\n",
    "env = gym.make(ENV_NAME)\n",
    "epsilon = INITIAL_EPSILON\n",
    "STATE_DIM = env.observation_space.shape[0]\n",
    "ACTION_DIM = env.action_space.n\n",
    "\n",
    "\n",
    "\n",
    "# Placeholders\n",
    "# -- DO NOT MODIFY --\n",
    "state_in = tf.placeholder(\"float\", [None, STATE_DIM])\n",
    "action_in = tf.placeholder(\"float\", [None, ACTION_DIM])\n",
    "target_in = tf.placeholder(\"float\", [None])\n",
    "\n",
    "# TODO: Define Network Graph\n",
    "with tf.variable_scope(\"DQN\"):\n",
    "    #Network architecture\n",
    "    # batch_data.\n",
    "    input_data = tf.placeholder(dtype=tf.float32, shape=[None, 1, 4], name=\"input_data\")\n",
    "    # input_data = tf.transpose(input_data, [2,1,0])\n",
    "    lstm_rnn_cell = tf.contrib.cudnn_rnn.CudnnCompatibleLSTMCell(\n",
    "    num_units=HIDDEN_UNITS)\n",
    "\n",
    "\n",
    "    lstm_c_fw = [tf.contrib.rnn.DropoutWrapper(lstm_rnn_cell, output_keep_prob=dropout_keep_prob)]\n",
    "\n",
    "\n",
    "    lstm_c_bw = [tf.contrib.rnn.DropoutWrapper(lstm_rnn_cell, output_keep_prob=dropout_keep_prob)]\n",
    "\n",
    "\n",
    "    outputs, _, _ = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(cells_fw=lstm_c_fw,cells_bw=lstm_c_bw,inputs=input_data,dtype=tf.float32, time_major=False)\n",
    "\n",
    "    outputs = tf.layers.dense(\n",
    "                                inputs=outputs,\n",
    "                                units=ACTION_DIM,\n",
    "                                kernel_initializer=tf.initializers.random_uniform(-.01, 0.01),\n",
    "                                # activation=tf.nn.relu,\n",
    "                                name=\"layer1\")\n",
    "    \n",
    "    q_values = tf.layers.dense(\n",
    "                                inputs=state_in,\n",
    "                                units=ACTION_DIM,\n",
    "                                kernel_initializer=tf.initializers.random_uniform(-.01, 0.01),\n",
    "                                # activation=tf.nn.relu,\n",
    "                                name=\"layerq\")\n",
    "    # q_values = outputs[:,-1,:]\n",
    "    '''q_values is a vector of the q_values for each action given the current state. e.g. [0.1, -0.6]\n",
    "    q_action is just the q_value for the specific action taken. Mathematically, actions will be a one_hot vector like [0, 1], so the multiplication of actions with values will be:\n",
    "    q_action = [0.1, -0.6] * [0, 1] = [0, -0.6]\n",
    "    tf.reduce_sum() has the function of compiling this down to the Q(s,a)(q value), for that specific state because it is currently like this:\n",
    "    [Q(s, action 1), Q(s, action 2)]\n",
    "    Therefore, q_action becomes: -0.6\n",
    "    '''\n",
    "    q_action = tf.reduce_sum(tf.multiply(outputs[:,-1,:], action_in), axis=1)\n",
    "    # loss = tf.reduce_mean(tf.square(target_in - q_action))\n",
    "    l_loss = tf.reduce_mean(tf.square(target_in - q_action))\n",
    "    optimiser = tf.train.AdamOptimizer(learning_rate=0.01).minimize(l_loss)\n",
    "\n",
    "class BatchMemory:\n",
    "    def __init__(self, memory):\n",
    "        self.memory = []\n",
    "        self.max_memory = memory\n",
    "        self.update_runner = 0\n",
    "\n",
    "    def add(self, elem):\n",
    "        if self.size > self.max_memory:\n",
    "            self.memory[self.update_runner] = elem\n",
    "        else:\n",
    "            self.memory.append(elem)\n",
    "\n",
    "    def get_single(self):\n",
    "        return self.memory[self.rand_access]  \n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "        if batch_size > self.size:\n",
    "            return random.sample(self.memory, self.size)\n",
    "        return random.sample(self.memory, batch_size-1)\n",
    "\n",
    "    @property\n",
    "    def size(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "    @property\n",
    "    def rand_access(self):\n",
    "        return random.randint(0,self.size) if self.size == 0 else random.randint(0,self.size-1)\n",
    "\n",
    "# TODO: Network outputs\n",
    "\n",
    "\n",
    "# TODO: Loss/Optimizer Definition\n",
    "\n",
    "memory = BatchMemory(1000)\n",
    "# Start session - Tensorflow housekeeping\n",
    "session = tf.InteractiveSession()\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "# -- DO NOT MODIFY ---\n",
    "def explore(state, epsilon):\n",
    "    \"\"\"\n",
    "    Exploration function: given a state and an epsilon value,\n",
    "    and assuming the network has already been defined, decide which action to\n",
    "    take using e-greedy exploration based on the current q-value estimates.\n",
    "    \"\"\"\n",
    "    Q_estimates = outputs.eval(feed_dict={\n",
    "        input_data: state\n",
    "    })\n",
    "    if random.random() <= epsilon:\n",
    "        action = random.randint(0, ACTION_DIM - 1)\n",
    "    else:\n",
    "        action = np.argmax(Q_estimates)\n",
    "    one_hot_action = np.zeros(ACTION_DIM)\n",
    "    one_hot_action[action] = 1\n",
    "    return one_hot_action\n",
    "\n",
    "ave_reward = 0\n",
    "# Main learning loop\n",
    "next_s_np = []\n",
    "reward_np = []\n",
    "done_np = []\n",
    "action_np = []\n",
    "state_np = []\n",
    "\n",
    "# frame = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for episode in range(EPISODE):\n",
    "\n",
    "    # initialize task\n",
    "    state = env.reset()\n",
    "\n",
    "    # Update epsilon once per episode\n",
    "    epsilon -= epsilon / EPSILON_DECAY_STEPS\n",
    "    \n",
    "    # Move through env according to e-greedy policy\n",
    "    for step in range(STEP):\n",
    "        # print(state.shape)\n",
    "        s=np.reshape(state, [1, 1, state.shape[0]])\n",
    "        # print(s.shape)\n",
    "        action = explore(s, epsilon)\n",
    "        next_state, reward, done, _ = env.step(np.argmax(action))\n",
    "        n = np.reshape(next_state, [1, 1, next_state.shape[0]])\n",
    "        nextstate_q_values = outputs.eval(feed_dict={\n",
    "            input_data: n\n",
    "        })\n",
    "        \n",
    "        next_s_np.append(next_state)\n",
    "        reward_np.append(reward)\n",
    "        done_np.append(done)\n",
    "        action_np.append(action)\n",
    "        state_np.append(state)\n",
    "\n",
    "        # TODO: Calculate the target q-value.\n",
    "        # hint1: Bellman\n",
    "        # hint2: consider if the episode has terminated\n",
    "        \n",
    "\n",
    "        # Do one training step\n",
    "        # session.run([optimiser], feed_dict={\n",
    "        #     target_in: [target],\n",
    "        #     action_in: [action],\n",
    "        #     state_in: [state]\n",
    "        # })\n",
    "\n",
    "        # Update\n",
    "        state = next_state\n",
    "        if done:\n",
    "            record = [np.array(entry) for entry in [state_np, action_np, reward_np, next_s_np, done_np]]\n",
    "            # print(record)\n",
    "            memory.add(record)\n",
    "            break\n",
    "    if ave_reward < 195:\n",
    "        \n",
    "        batch_ = memory.get_batch(batch_size)\n",
    "        for k in range(0,len(batch_)):\n",
    "            \n",
    "            batch = batch_[0]\n",
    "            # print(batch)\n",
    "            # [print(i) for i in batch]\n",
    "            # state_b = np.stack(*[k[0] for k in batch], axis=0)\n",
    "            state_b =  batch[0]\n",
    "            # print(state_b)\n",
    "            state_b = np.reshape(state_b, [state_b.shape[0],1, state_b.shape[1]])\n",
    "            # print('makin it round')\n",
    "            action_b = batch[1] \n",
    "            \n",
    "            action_b = np.reshape(action_b, [action_b.shape[0],action_b.shape[1]])\n",
    "            reward_b = batch[2] \n",
    "            next_state_b = batch[3] \n",
    "            next_state_b = np.reshape(next_state_b, [next_state_b.shape[0],1, next_state_b.shape[1]])\n",
    "            finish_b = batch[4] \n",
    "            feed = {input_data: next_state_b}\n",
    "            q_next_state = session.run(outputs, feed)\n",
    "\n",
    "            target_q = []\n",
    "            # print('preloop')\n",
    "            for k in range(0, len(batch[0])):\n",
    "                if finish_b[k]:\n",
    "                    target_q.append(reward_b[k])\n",
    "                else:\n",
    "                    temp = reward_b[k] + GAMMA*np.max(q_next_state[k])\n",
    "                    target_q.append(temp)\n",
    "\n",
    "\n",
    "            # print(target_q)\n",
    "            # print(action_b)\n",
    "            # print(state_b)\n",
    "\n",
    "            session.run([optimiser], feed_dict={\n",
    "                target_in: target_q,\n",
    "                action_in: action_b,\n",
    "                input_data: state_b\n",
    "            })\n",
    "        \n",
    "    # Test and view sample runs - can disable render to `~save time\n",
    "    # -- DO NOT MODIFY --\n",
    "    if (episode % TEST_FREQUENCY == 0 and episode != 0):\n",
    "        total_reward = 0\n",
    "        for i in range(TEST):\n",
    "            state = env.reset()\n",
    "            for j in range(STEP):\n",
    "                # env.render()\n",
    "                action = np.argmax(q_values.eval(feed_dict={\n",
    "                    state_in: [state]\n",
    "                }))\n",
    "                state, reward, done, _ = env.step(action)\n",
    "                total_reward += reward\n",
    "                if done:\n",
    "                    break\n",
    "        ave_reward = total_reward / TEST\n",
    "        print('episode:', episode, 'epsilon:', epsilon, 'Evaluation '\n",
    "                                                        'Average Reward:', ave_reward)\n",
    "\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
